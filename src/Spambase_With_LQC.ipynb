{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "import ast\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain, combinations\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    confusion_matrix, \n",
    "    ConfusionMatrixDisplay, \n",
    "    classification_report\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable ConvergenceWarnings\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read The Spambase Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '.../data/spambase.data'  # Adjust the path as needed\n",
    "df = pd.read_csv(file_path, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].to_numpy()\n",
    "y = df.iloc[:, -1].to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the split data function for partitioning the data among the clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Spambase.split_data import split_data_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the corruption data functions for corrupting the client's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Spambase.corrupt_data_spambase import corrupt_data, corrupt_clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the aggregate functions for aggregating the cclient's models in FedLR and FedFor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.aggregate_functions import aggregate_lr_models, FederatedForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the decision tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.DecisionTree import DecisionTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the evaluate coalitions function for evaluating all possible coalitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluate_coalitions import evaluate_coalitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the finding nash equilibria function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Nash import find_nash_equilibria_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_partitions(X_train, y_train, n_clients, random_seed, corruption_settings, noise_std, corrupt_client_indices, corrupt_function):\n",
    "    partitions = split_data_equal(X_train, y_train, n_clients=n_clients, shuffle=True, random_seed=random_seed)\n",
    "    corrupted_partitions, _ = corrupt_clients(\n",
    "        corrupt_function, partitions, corrupt_client_indices,\n",
    "        corruption_prob=corruption_settings.get(\"corruption_prob\", 0.6),\n",
    "        nan_prob=corruption_settings.get(\"nan_prob\", 0.5),\n",
    "        noise_std=noise_std,\n",
    "        label_corruption_prob=corruption_settings.get(\"label_corruption_prob\", 0.1),\n",
    "        base_seed=random_seed\n",
    "    )\n",
    "    normalized_partitions = []\n",
    "    for X_part, y_part in corrupted_partitions:\n",
    "        local_scaler = StandardScaler()\n",
    "        X_norm = local_scaler.fit_transform(X_part)\n",
    "        normalized_partitions.append((X_norm, y_part))\n",
    "    return normalized_partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for FedLR training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_fedlr(partitions, random_seed, X_test, y_test, max_iter):\n",
    "\n",
    "    client_models = []\n",
    "    client_global_accuracies = []\n",
    "    \n",
    "    for X_i, y_i in partitions:\n",
    "        # Clean data: remove rows with NaN values.\n",
    "        nan_mask = ~np.isnan(X_i).any(axis=1)\n",
    "        X_clean = X_i[nan_mask]\n",
    "        y_clean = y_i[nan_mask]\n",
    "        if len(y_clean) == 0:\n",
    "            client_models.append(None)\n",
    "            client_global_accuracies.append(None)\n",
    "            continue\n",
    "        \n",
    "        model = LogisticRegression(random_state=random_seed, max_iter=max_iter)\n",
    "        try:\n",
    "            model.fit(X_clean, y_clean)\n",
    "            client_models.append(model)\n",
    "            client_global_accuracies.append(model.score(X_test, y_test))\n",
    "        except Exception as e:\n",
    "            client_models.append(None)\n",
    "            client_global_accuracies.append(None)\n",
    "    \n",
    "    return client_models, client_global_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for FedFor training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_fedfor(partitions, X_test, y_test, max_depth):\n",
    "\n",
    "    client_models = []\n",
    "    client_global_accuracies = {}\n",
    "    \n",
    "    for i, (X_i, y_i) in enumerate(partitions):\n",
    "        model = DecisionTreeClassifier(max_depth=max_depth, random_state=np.random.randint(0, 100000))\n",
    "        model.fit(X_i, y_i)\n",
    "        client_models.append(model)\n",
    "        y_pred = model.predict(X_test)\n",
    "        client_global_accuracies[i] = np.mean(y_pred == y_test)  # equivalent to accuracy_score\n",
    "    return client_models, client_global_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for training procedure in each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(approach, trial_seed, n_clients, X_train, y_train, X_test, y_test,\n",
    "              hyper_param, noise_std, corrupt_client_indices, corruption_settings, corrupt_function):\n",
    "\n",
    "    partitions = prepare_partitions(\n",
    "        X_train, y_train, n_clients, trial_seed, corruption_settings,\n",
    "        noise_std, corrupt_client_indices, corrupt_function\n",
    "    )\n",
    "    if approach == 'fedlr':\n",
    "        client_models, client_global_acc = train_models_fedlr(partitions, trial_seed, X_test, y_test, max_iter=hyper_param)\n",
    "        df_results = evaluate_coalitions(client_models, client_global_acc, n_clients, aggregate_lr_models, X_test, y_test, corrupt_client_indices, approach='fedlr')\n",
    "        return df_results, client_global_acc\n",
    "    elif approach == 'fedfor':\n",
    "        client_models, client_global_acc = train_models_fedfor(partitions, X_test, y_test, max_depth=hyper_param)\n",
    "        df_results = evaluate_coalitions(client_models, client_global_acc, n_clients, FederatedForest, X_test, y_test, corrupt_client_indices, approach='fedfor')\n",
    "        return df_results, client_global_acc\n",
    "    else:\n",
    "        raise ValueError(\"Unknown approach specified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function to perform all processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(approach, n_trials, n_clients, hyper_params, partitions_corrupted,\n",
    "                   corrupt_client_indices, X_train, y_train, X_test, y_test,\n",
    "                   noise_std, save_dir, corrupt_function=corrupt_data, corruption_settings=None,\n",
    "                   base_random_seed=42):\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    all_details = []\n",
    "    all_client_accuracies = []\n",
    "    \n",
    "    # Default corruption settings if none provided.\n",
    "    if corruption_settings is None:\n",
    "        corruption_settings = {'corruption_prob': 0.6, 'nan_prob': 0.5, 'label_corruption_prob': 0.1}\n",
    "    \n",
    "    for hyper_param in hyper_params:\n",
    "        #if verbose:\n",
    "            #print(f\"\\nRunning experiment for hyper_param = {hyper_param}\")\n",
    "        nash_counts = Counter()\n",
    "        details_for_this_param = []\n",
    "        client_accuracy_details = []\n",
    "        \n",
    "        # If no corrupted client indices provided, choose randomly based on partitions_corrupted.\n",
    "        if corrupt_client_indices is None:\n",
    "            corrupt_client_indices = np.random.choice(n_clients, size=partitions_corrupted, replace=False)\n",
    "        \n",
    "        for trial in range(n_trials):\n",
    "            rand_component = random.randint(0, 500)\n",
    "            trial_seed = base_random_seed + trial + int(1000 * hyper_param) + 2 * rand_component\n",
    "            #if verbose:\n",
    "                #print(f\" Trial {trial+1}/{n_trials}, Seed: {trial_seed}\")\n",
    "            \n",
    "            if approach == 'fedlr':\n",
    "                df_results, client_global_acc = run_trial(\n",
    "                    approach, trial_seed, n_clients, X_train, y_train, X_test, y_test,\n",
    "                    hyper_param, noise_std, corrupt_client_indices, corruption_settings, corrupt_function\n",
    "                )\n",
    "            else:\n",
    "                df_results, client_global_acc = run_trial(\n",
    "                    approach, trial_seed, n_clients, X_train, y_train, X_test, y_test,\n",
    "                    hyper_param, noise_std, corrupt_client_indices, corruption_settings, corrupt_function\n",
    "                )\n",
    "            \n",
    "            # Identify Nash equilibria (using your pre-defined function)\n",
    "            df_nash = find_nash_equilibria_v2(df_results.reset_index())\n",
    "            for coalition in df_nash['Combination']:\n",
    "                nash_counts[coalition] += 1\n",
    "            df_nash['Trial'] = trial + 1\n",
    "            df_nash['Noise Std'] = noise_std\n",
    "            df_nash['Corrupted Clients'] = len(corrupt_client_indices)\n",
    "            df_nash['Max Iter or Depth'] = hyper_param\n",
    "            details_for_this_param.append(df_nash)\n",
    "            \n",
    "            # Collect client accuracy details for this trial.\n",
    "            trial_acc = {\n",
    "                'Trial': trial + 1,\n",
    "                'Max Iter or Depth': hyper_param,\n",
    "                'Noise Std': noise_std,\n",
    "                'Corrupted Clients': len(corrupt_client_indices)\n",
    "            }\n",
    "            for j in range(n_clients):\n",
    "                col_name = f'Client {j+1} Accuracy'\n",
    "                if j in corrupt_client_indices:\n",
    "                    col_name += \" (low-quality client)\"\n",
    "                if approach == 'fedlr':\n",
    "                    trial_acc[col_name] = client_global_acc[j] if client_global_acc[j] is not None else np.nan\n",
    "                else:\n",
    "                    trial_acc[col_name] = client_global_acc.get(j, np.nan)\n",
    "            client_accuracy_details.append(trial_acc)\n",
    "        \n",
    "        # Aggregate details for the current hyper parameter.\n",
    "        df_details = pd.concat(details_for_this_param, ignore_index=True)\n",
    "        df_client_accuracy = pd.DataFrame(client_accuracy_details)\n",
    "        df_combined = df_details.merge(\n",
    "            df_client_accuracy,\n",
    "            on=['Trial', 'Max Iter or Depth', 'Noise Std', 'Corrupted Clients'],\n",
    "            how='left'\n",
    "        )\n",
    "        all_details.append(df_combined)\n",
    "    \n",
    "    final_details_df = pd.concat(all_details, ignore_index=True)\n",
    "    details_path = os.path.join(save_dir, f\"Nash_Equilibrium_Details_{approach}_noise_{noise_std}_c{len(corrupt_client_indices)}.csv\")\n",
    "    if not os.path.exists(details_path):\n",
    "        final_details_df.to_csv(details_path, index=False)\n",
    "    \n",
    "    return final_details_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FedLR Spambase: Approximately 350 samples for each client:\n",
    "noise std values = [0.1 , 0.3 , 0.5 , 0.7 ,1 , 2 , 3 , 4 , 5] \n",
    "and low-quality clients counts = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_std_values = [0.1, 0.3, 0.5, 0.7, 1, 2 , 3 , 4 ,5]\n",
    "corrupted_clients_counts = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]        \n",
    "save_dir = \".../results/FedLR_Spambase_LQC_0_to_10\"\n",
    "max_iters = [10,100]\n",
    "results = {noise: [] for noise in noise_std_values}\n",
    "\n",
    "for noise in noise_std_values:\n",
    "    for cc in corrupted_clients_counts:\n",
    "        client_indices = list(range(cc))\n",
    "        custom_corrupt_data = lambda X, y, corruption_prob=0.6, nan_prob=0.5, noise_std=noise, label_corruption_prob=0.1, random_seed=None: \\\n",
    "            corrupt_data(X, y, corruption_prob, nan_prob, noise_std, label_corruption_prob, random_seed)\n",
    "\n",
    "        results_fedlr = run_experiment(\n",
    "            approach='fedlr',\n",
    "            n_trials=50,\n",
    "            n_clients=10,\n",
    "            hyper_params=[10, 100], \n",
    "            partitions_corrupted=cc,\n",
    "            corrupt_client_indices=client_indices,  \n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test_scaled,\n",
    "            y_test=y_test,\n",
    "            noise_std=noise,\n",
    "            save_dir=save_dir,\n",
    "            corrupt_function=corrupt_data\n",
    "        )\n",
    "        \n",
    "\n",
    "        occurrence_count = (results_fedlr['Combination'] == '1111111111').sum()\n",
    "        results[noise].append(occurrence_count)\n",
    "        print(f\"Noise Std: {noise}, Bad Clients: {cc}, max_iters: {max_iters}, Occurrences: {occurrence_count}\")\n",
    "\n",
    "results_df = pd.DataFrame(results, index=corrupted_clients_counts)\n",
    "results_df.index.name = \"Number of Bad Clients\"\n",
    "results_csv_path = os.path.join(save_dir, \"nash_occurrence_results.csv\")\n",
    "results_df.to_csv(results_csv_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FedLR Spambase: Approximately 350 samples for each client:\n",
    "noise std values = [0.1 ] \n",
    "and low-quality clients counts = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] and 1000 Trials in Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_std_values = [0.1]\n",
    "corrupted_clients_counts = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]        \n",
    "save_dir = \".../results/FedLR_Spambase_LQC_0_to_10_with_1000Trials\"\n",
    "max_iters = [10,100]\n",
    "results = {noise: [] for noise in noise_std_values}\n",
    "\n",
    "for noise in noise_std_values:\n",
    "    for cc in corrupted_clients_counts:\n",
    "        client_indices = list(range(cc))\n",
    "        custom_corrupt_data = lambda X, y, corruption_prob=0.6, nan_prob=0.5, noise_std=noise, label_corruption_prob=0.1, random_seed=None: \\\n",
    "            corrupt_data(X, y, corruption_prob, nan_prob, noise_std, label_corruption_prob, random_seed)\n",
    "\n",
    "        results_fedlr = run_experiment(\n",
    "            approach='fedlr',\n",
    "            n_trials=500,\n",
    "            n_clients=10,\n",
    "            hyper_params=[10, 100], \n",
    "            partitions_corrupted=cc,\n",
    "            corrupt_client_indices=client_indices,  \n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test_scaled,\n",
    "            y_test=y_test,\n",
    "            noise_std=noise,\n",
    "            save_dir=save_dir,\n",
    "            corrupt_function=corrupt_data\n",
    "        )\n",
    "        \n",
    "\n",
    "        occurrence_count = (results_fedlr['Combination'] == '1111111111').sum()\n",
    "        results[noise].append(occurrence_count)\n",
    "        print(f\"Noise Std: {noise}, Bad Clients: {cc}, max_iters: {max_iters}, Occurrences: {occurrence_count}\")\n",
    "\n",
    "results_df = pd.DataFrame(results, index=corrupted_clients_counts)\n",
    "results_df.index.name = \"Number of Bad Clients\"\n",
    "results_csv_path = os.path.join(save_dir, \"nash_occurrence_results.csv\")\n",
    "results_df.to_csv(results_csv_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FedFor Spambase: Approximately 350 samples for each client:\n",
    "noise std values = [0.1 , 0.3 , 0.5 , 0.7 ,1 , 2 , 3 , 4 , 5] \n",
    "and low-quality clients counts = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_std_values = [0.1, 0.3, 0.5, 0.7, 1, 2 , 3 , 4 ,5]\n",
    "corrupted_clients_counts = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  \n",
    "max_depths = [10,100]      \n",
    "save_dir = \".../results/FedFor_Spambase_LQC_0_to_10\"\n",
    "\n",
    "results = {noise: [] for noise in noise_std_values}\n",
    "\n",
    "for noise in noise_std_values:\n",
    "    for cc in corrupted_clients_counts:\n",
    "        client_indices = list(range(cc))\n",
    "        custom_corrupt_data = lambda X, y, corruption_prob=0.6, nan_prob=0.5, noise_std=noise, label_corruption_prob=0.1, random_seed=None: \\\n",
    "            corrupt_data(X, y, corruption_prob, nan_prob, noise_std, label_corruption_prob, random_seed)\n",
    "\n",
    "        results_fedfor = run_experiment(\n",
    "            approach='fedfor',\n",
    "            n_trials=50,\n",
    "            n_clients=10,\n",
    "            hyper_params = max_depths,  \n",
    "            partitions_corrupted=cc,\n",
    "            corrupt_client_indices=client_indices,  \n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test_scaled,\n",
    "            y_test=y_test,\n",
    "            noise_std=noise,\n",
    "            save_dir=save_dir,\n",
    "            corrupt_function=corrupt_data\n",
    "        )\n",
    "        \n",
    "\n",
    "        occurrence_count = (results_fedfor['Combination'] == '1111111111').sum()\n",
    "        results[noise].append(occurrence_count)\n",
    "        print(f\"Noise Std: {noise}, Bad Clients: {cc}, max_depths: {max_depths}, Occurrences: {occurrence_count}\")\n",
    "\n",
    "results_df = pd.DataFrame(results, index=corrupted_clients_counts)\n",
    "results_df.index.name = \"Number of Bad Clients\"\n",
    "results_csv_path = os.path.join(save_dir, \"nash_occurrence_results.csv\")\n",
    "results_df.to_csv(results_csv_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FedFor Spambase: Approximately 350 samples for each client:\n",
    "noise std values = [0.1 ] \n",
    "and low-quality clients counts = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] and 1000 Trials in Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_std_values = [0.1]\n",
    "corrupted_clients_counts = [0,1,2,3,4,5,6,7, 8, 9, 10]  \n",
    "max_depths = [10,100]      \n",
    "save_dir = \".../results/FedFor_Spambase_LQC_0_to_10_with_1000Trials\"\n",
    "\n",
    "results = {noise: [] for noise in noise_std_values}\n",
    "\n",
    "for noise in noise_std_values:\n",
    "    for cc in corrupted_clients_counts:\n",
    "        client_indices = list(range(cc))\n",
    "        custom_corrupt_data = lambda X, y, corruption_prob=0.6, nan_prob=0.5, noise_std=noise, label_corruption_prob=0.1, random_seed=None: \\\n",
    "            corrupt_data(X, y, corruption_prob, nan_prob, noise_std, label_corruption_prob, random_seed)\n",
    "\n",
    "        results_fedfor = run_experiment(\n",
    "            approach='fedfor',\n",
    "            n_trials=500,\n",
    "            n_clients=10,\n",
    "            hyper_params = max_depths,  \n",
    "            partitions_corrupted=cc,\n",
    "            corrupt_client_indices=client_indices,  \n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test_scaled,\n",
    "            y_test=y_test,\n",
    "            noise_std=noise,\n",
    "            save_dir=save_dir,\n",
    "            corrupt_function=corrupt_data\n",
    "        )\n",
    "        \n",
    "\n",
    "        occurrence_count = (results_fedfor['Combination'] == '1111111111').sum()\n",
    "        results[noise].append(occurrence_count)\n",
    "        print(f\"Noise Std: {noise}, Bad Clients: {cc}, max_depths: {max_depths}, Occurrences: {occurrence_count}\")\n",
    "\n",
    "results_df = pd.DataFrame(results, index=corrupted_clients_counts)\n",
    "results_df.index.name = \"Number of Bad Clients\"\n",
    "results_csv_path = os.path.join(save_dir, \"nash_occurrence_results.csv\")\n",
    "results_df.to_csv(results_csv_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
