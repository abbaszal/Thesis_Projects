{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import copy\n",
    "import ast\n",
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain, combinations\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable ConvergenceWarnings\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read The Global HuGaDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File patterns.\n",
    "train_files_pattern = \".../data/metadata/train_{i:02d}.csv\"\n",
    "test_files_pattern = \".../data/metadata/test_{i:02d}.csv\"\n",
    "\n",
    "# Concatenate all training files.\n",
    "df_train_global = pd.concat([\n",
    "    pd.read_csv(train_files_pattern.format(i=i)) for i in range(1, 11)\n",
    "]).dropna()\n",
    "\n",
    "# Concatenate all testing files.\n",
    "df_test_global = pd.concat([\n",
    "    pd.read_csv(test_files_pattern.format(i=i)) for i in range(1, 11)\n",
    "]).dropna()\n",
    "\n",
    "# Split features and labels.\n",
    "X_train_global = df_train_global.drop('act', axis=1)\n",
    "y_train_global = df_train_global['act']\n",
    "\n",
    "X_test_global = df_test_global.drop('act', axis=1)\n",
    "y_test_global = df_test_global['act']\n",
    "\n",
    "# Encode labels.\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_global = label_encoder.fit_transform(y_train_global)\n",
    "y_test_global = label_encoder.transform(y_test_global)\n",
    "\n",
    "# Scale features.\n",
    "scaler_global = StandardScaler()\n",
    "X_train_global_scaled = scaler_global.fit_transform(X_train_global)\n",
    "X_test_global_scaled  = scaler_global.transform(X_test_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsampled test set shape: (950, 38)\n"
     ]
    }
   ],
   "source": [
    "# Create a stratified subsample of the test set to speed up the runtime.\n",
    "subsample_size = 950  \n",
    "X_test_global_scaled, _, y_test_global, _ = train_test_split(\n",
    "    X_test_global_scaled, y_test_global,\n",
    "    train_size=subsample_size,\n",
    "    random_state=42,\n",
    "    stratify=y_test_global\n",
    ")\n",
    "print(\"Subsampled test set shape:\", X_test_global_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the corruption data functions for corrupting the client's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.HuGaDB.corrupt_data_hugadb import corrupt_data, corrupt_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the prepare partitions function to prepare client data, with possible data corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.HuGaDB.prepare_partitions import prepare_partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the aggregate functions for aggregating the cclient's models in FedLR and FedFor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.aggregate_functions import aggregate_lr_models, FederatedForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the decision tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.DecisionTree import DecisionTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the evaluate coalitions function for evaluating all possible coalitions ~ 1023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluate_coalitions import evaluate_coalitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the finding nash equilibria function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Nash import find_nash_equilibria_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for FedLR training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_fedlr(n_clients, trial_seed, sample_size, num_corrupted_clients,\n",
    "                       train_files_pattern, corruption_params, label_corruption_prob, hyper_param):\n",
    "    \"\"\"\n",
    "    Trains models for all clients using FedLR (Logistic Regression).\n",
    "    Returns a list of trained models and a dict of global accuracies.\n",
    "    \"\"\"\n",
    "    client_models = []\n",
    "    client_global_accuracies = {}\n",
    "    for client_idx in range(1, n_clients + 1):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        \n",
    "        X_train_scaled, y_train = prepare_partitions(\n",
    "            client_idx, trial_seed, sample_size, num_corrupted_clients,\n",
    "            train_files_pattern, corruption_params, label_corruption_prob,label_encoder\n",
    "        )\n",
    "        \n",
    "        model = LogisticRegression(random_state=trial_seed, max_iter=hyper_param)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        acc_global = accuracy_score(y_test_global, model.predict(X_test_global_scaled))\n",
    "        \n",
    "        client_models.append(model)\n",
    "        client_global_accuracies[client_idx - 1] = acc_global\n",
    "    return client_models, client_global_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for FedFor training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_fedfor(n_clients, trial_seed, sample_size, num_corrupted_clients,\n",
    "                        train_files_pattern, corruption_params, label_corruption_prob, hyper_param):\n",
    "\n",
    "    client_models = []\n",
    "    client_global_accuracies = {}\n",
    "    for client_idx in range(1, n_clients + 1):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        \n",
    "        X_train_scaled, y_train = prepare_partitions(\n",
    "            client_idx, trial_seed, sample_size, num_corrupted_clients,\n",
    "            train_files_pattern, corruption_params, label_corruption_prob , label_encoder\n",
    "        )\n",
    "        \n",
    "        model = DecisionTree(max_depth=hyper_param, random_state=trial_seed)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        acc_global = accuracy_score(y_test_global, model.predict(X_test_global_scaled))\n",
    "        \n",
    "        client_models.append(model)\n",
    "        client_global_accuracies[client_idx - 1] = acc_global\n",
    "    return client_models, client_global_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for training procedure in each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(approach, n_clients, trial_seed, sample_size, num_corrupted_clients,\n",
    "              train_files_pattern, corruption_params, label_corruption_prob, hyper_param,\n",
    "              aggregator_func, X_test, y_test, corrupt_client_indices):\n",
    "    \n",
    "    if approach == 'fedlr':\n",
    "        client_models, client_global_accuracies = train_models_fedlr(\n",
    "            n_clients, trial_seed, sample_size, num_corrupted_clients,\n",
    "            train_files_pattern, corruption_params, label_corruption_prob, hyper_param\n",
    "        )\n",
    "    else:\n",
    "        client_models, client_global_accuracies = train_models_fedfor(\n",
    "            n_clients, trial_seed, sample_size, num_corrupted_clients,\n",
    "            train_files_pattern, corruption_params, label_corruption_prob, hyper_param\n",
    "        )\n",
    "    \n",
    "    df_results = evaluate_coalitions(\n",
    "        client_models, client_global_accuracies, n_clients, aggregator_func,\n",
    "        X_test, y_test, corrupt_client_indices, approach\n",
    "    )\n",
    "    \n",
    "    df_nash = find_nash_equilibria_v2(df_results.reset_index())\n",
    "    return df_results, df_nash, client_global_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function to perform all processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(approach, n_trials, n_clients, hyper_params, noise_std, num_corrupted_clients,\n",
    "                   save_dir, sample_size=350, base_random_seed=42,\n",
    "                   data_root=\"/Users/abbaszal/Documents/Thesis_Project_Spambase/data/metadata\"):\n",
    "\n",
    "\n",
    "    train_files_pattern = os.path.join(data_root, \"train_{:02d}.csv\")\n",
    "    test_files_pattern = os.path.join(data_root, \"test_{:02d}.csv\")\n",
    "    \n",
    "\n",
    "    corruption_params = {\n",
    "        'corruption_prob': 0.6,\n",
    "        'nan_prob': 0.5,\n",
    "        'noise_std': noise_std\n",
    "    }\n",
    "    label_corruption_prob = 0.1\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    all_details = []\n",
    "    \n",
    "\n",
    "    corrupt_client_indices = list(range(num_corrupted_clients))\n",
    "    \n",
    "\n",
    "    if approach == 'fedlr':\n",
    "        aggregator_func = aggregate_lr_models  \n",
    "    else:\n",
    "        aggregator_func = lambda: FederatedForest() \n",
    "    \n",
    "    client_accuracy_details_all = []\n",
    "    details_for_all_hyper = []\n",
    "    \n",
    "    for hyper_param in hyper_params:\n",
    "        details_for_this_param = []\n",
    "        client_accuracy_details = []\n",
    "        nash_counts = Counter()\n",
    "        \n",
    "        for trial in range(n_trials):\n",
    "            rand_component = random.randint(0, 500)\n",
    "            trial_seed = base_random_seed + trial + int(1000 * hyper_param) + rand_component\n",
    "            random.seed(trial_seed)\n",
    "            np.random.seed(trial_seed)\n",
    "            \n",
    "            df_results, df_nash, client_global_accuracies = run_trial(\n",
    "                approach, n_clients, trial_seed, sample_size, num_corrupted_clients,\n",
    "                train_files_pattern, corruption_params, label_corruption_prob, hyper_param,\n",
    "                aggregator_func, X_test_global_scaled, y_test_global, corrupt_client_indices\n",
    "            )\n",
    "            \n",
    " \n",
    "            for coalition in df_nash['Combination']:\n",
    "                nash_counts[coalition] += 1\n",
    "            df_nash['Trial'] = trial + 1\n",
    "            df_nash['Noise Std'] = noise_std\n",
    "            df_nash['Corrupted Clients'] = num_corrupted_clients\n",
    "            df_nash['Max Iter or Depth'] = hyper_param\n",
    "            details_for_this_param.append(df_nash)\n",
    "            \n",
    "            trial_acc = {\n",
    "                'Trial': trial + 1,\n",
    "                'Max Iter or Depth': hyper_param,\n",
    "                'Noise Std': noise_std,\n",
    "                'Corrupted Clients': num_corrupted_clients\n",
    "            }\n",
    "            for j in range(n_clients):\n",
    "                col_name = f'Client {j+1} Accuracy'\n",
    "                if j in corrupt_client_indices:\n",
    "                    col_name += \"(low-quality client)\"\n",
    "                trial_acc[col_name] = client_global_accuracies[j] if client_global_accuracies[j] is not None else np.nan\n",
    "            client_accuracy_details.append(trial_acc)\n",
    "        \n",
    "        df_details = pd.concat(details_for_this_param, ignore_index=True)\n",
    "        df_client_accuracy = pd.DataFrame(client_accuracy_details)\n",
    "        df_combined = df_details.merge(\n",
    "            df_client_accuracy,\n",
    "            on=['Trial', 'Max Iter or Depth', 'Noise Std', 'Corrupted Clients'],\n",
    "            how='left'\n",
    "        )\n",
    "        all_details.append(df_combined)\n",
    "        \n",
    "\n",
    "    \n",
    "    final_details_df = pd.concat(all_details, ignore_index=True)\n",
    "    details_path = os.path.join(save_dir, f\"Nash_Equilibrium_Details_{approach}_noise_{noise_std}_c{num_corrupted_clients}.csv\")\n",
    "    final_details_df.to_csv(details_path, index=False)\n",
    "    return final_details_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FedLR HuGaDB: 350 samples for each client:\n",
    "noise std values = [0.1 , 0.3 , 0.5 , 0.7 ,1 , 2 , 3 , 4 , 5] \n",
    "and low-quality clients counts = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_std_values = [0.1, 0.3, 0.5, 0.7, 1, 2, 3, 4, 5]\n",
    "corrupted_clients_counts = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "save_dir = \".../results/FedLR_HuGaDB_LQC_0_to_10_test\"\n",
    "\n",
    "\n",
    "results = {noise: [] for noise in noise_std_values}\n",
    "\n",
    "\n",
    "for noise in noise_std_values:\n",
    "    for cc in corrupted_clients_counts:\n",
    "        final_details_df = run_experiment(\n",
    "            approach='fedlr',\n",
    "            n_trials=50,\n",
    "            n_clients=10,\n",
    "            hyper_params=[10, 100],  \n",
    "            noise_std=noise,\n",
    "            num_corrupted_clients=cc,\n",
    "            save_dir=save_dir,\n",
    "            sample_size=350,        \n",
    "            base_random_seed=42,\n",
    "            data_root=\"/../data/metadata\"\n",
    "        )\n",
    "        \n",
    "\n",
    "        occurrence_count = (final_details_df['Combination'] == '1111111111').sum()\n",
    "        results[noise].append(occurrence_count)\n",
    "        print(f\"Noise Std: {noise}, Bad Clients: {cc}, Occurrences: {occurrence_count}\")\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results, index=corrupted_clients_counts)\n",
    "results_df.index.name = \"Number of Bad Clients\"\n",
    "results_csv_path = os.path.join(save_dir, \"nash_occurrence_results.csv\")\n",
    "results_df.to_csv(results_csv_path)\n",
    "print(f\"Results saved to {results_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FedLR HuGaDB: 350 samples for each client:\n",
    "noise std values = [0.1 ] \n",
    "and low-quality clients counts = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] and 1000 Trials in Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_std_values = [0.1]\n",
    "corrupted_clients_counts = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "save_dir = \".../results/FedLR_HuGaDB_LQC_0_to_10_with_1000Trials\"\n",
    "\n",
    "\n",
    "results = {noise: [] for noise in noise_std_values}\n",
    "\n",
    "\n",
    "for noise in noise_std_values:\n",
    "    for cc in corrupted_clients_counts:\n",
    "        final_details_df = run_experiment(\n",
    "            approach='fedlr',\n",
    "            n_trials=500,\n",
    "            n_clients=10,\n",
    "            hyper_params=[10, 100],  \n",
    "            noise_std=noise,\n",
    "            num_corrupted_clients=cc,\n",
    "            save_dir=save_dir,\n",
    "            sample_size=350,        \n",
    "            base_random_seed=42,\n",
    "            data_root=\"/.../data/metadata\"\n",
    "        )\n",
    "        \n",
    "\n",
    "        occurrence_count = (final_details_df['Combination'] == '1111111111').sum()\n",
    "        results[noise].append(occurrence_count)\n",
    "        print(f\"Noise Std: {noise}, Bad Clients: {cc}, Occurrences: {occurrence_count}\")\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results, index=corrupted_clients_counts)\n",
    "results_df.index.name = \"Number of Bad Clients\"\n",
    "results_csv_path = os.path.join(save_dir, \"nash_occurrence_results.csv\")\n",
    "results_df.to_csv(results_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FedFor HuGaDB: 350 samples for each client:\n",
    "noise std values = [0.1 , 0.3 , 0.5 , 0.7 ,1 , 2 , 3 , 4 , 5] \n",
    "and low-quality clients counts = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_std_values = [0.1, 0.3, 0.5, 0.7, 1, 2, 3, 4, 5]\n",
    "corrupted_clients_counts = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "save_dir = \".../results/FedFor_HuGaDB_LQC_0_to_10\"\n",
    "\n",
    "results = {noise: [] for noise in noise_std_values}\n",
    "\n",
    "\n",
    "for noise in noise_std_values:\n",
    "    for cc in corrupted_clients_counts:\n",
    "        final_details_df = run_experiment(\n",
    "            approach='fedfor',\n",
    "            n_trials=50,\n",
    "            n_clients=10,\n",
    "            hyper_params=[10, 100],  \n",
    "            noise_std=noise,\n",
    "            num_corrupted_clients=cc,\n",
    "            save_dir=save_dir,\n",
    "            sample_size=350,      \n",
    "            base_random_seed=42,\n",
    "            data_root=\"/.../data/metadata\"\n",
    "        )\n",
    "        \n",
    "\n",
    "        occurrence_count = (final_details_df['Combination'] == '1111111111').sum()\n",
    "        results[noise].append(occurrence_count)\n",
    "        print(f\"Noise Std: {noise}, Bad Clients: {cc}, Occurrences: {occurrence_count}\")\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results, index=corrupted_clients_counts)\n",
    "results_df.index.name = \"Number of Bad Clients\"\n",
    "results_csv_path = os.path.join(save_dir, \"nash_occurrence_results.csv\")\n",
    "results_df.to_csv(results_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FedFor HuGaDB: 350 samples for each client:\n",
    "noise std values = [0.1 ] \n",
    "and low-quality clients counts = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] and 1000 Trials in Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_std_values = [0.1]\n",
    "corrupted_clients_counts = [ 4, 5, 6, 7, 8, 9, 10]\n",
    "save_dir = \".../results/FedFor_HuGaDB_LQC_0_to_10_with_1000Trials\"\n",
    "\n",
    "results = {noise: [] for noise in noise_std_values}\n",
    "\n",
    "\n",
    "for noise in noise_std_values:\n",
    "    for cc in corrupted_clients_counts:\n",
    "        final_details_df = run_experiment(\n",
    "            approach='fedfor',\n",
    "            n_trials=500,\n",
    "            n_clients=10,\n",
    "            hyper_params=[10, 100],  \n",
    "            noise_std=noise,\n",
    "            \n",
    "            num_corrupted_clients=cc,\n",
    "            save_dir=save_dir,\n",
    "            sample_size=350,      \n",
    "            base_random_seed=42,\n",
    "            data_root=\".../data/metadata\"\n",
    "        )\n",
    "        \n",
    "\n",
    "        occurrence_count = (final_details_df['Combination'] == '1111111111').sum()\n",
    "        results[noise].append(occurrence_count)\n",
    "        print(f\"Noise Std: {noise}, Bad Clients: {cc}, Occurrences: {occurrence_count}\")\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results, index=corrupted_clients_counts)\n",
    "results_df.index.name = \"Number of Bad Clients\"\n",
    "results_csv_path = os.path.join(save_dir, \"nash_occurrence_results.csv\")\n",
    "results_df.to_csv(results_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
